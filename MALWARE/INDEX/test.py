import json
import os
import zipfile
import sys
import pandas as pd
from csv import writer

PRINT_MAX = 10000
reportFile = open('demo.txt', 'w')
report1File = open('demo1.txt', 'w')
data_row = []

def unzip_report(file_path):
    json_path = os.path.dirname(file_path) + os.sep
    json_file_path = os.path.splitext(file_path)[0] + '.json'

    zipfile.ZipFile(file_path, 'r').extractall(json_path)
    return json_file_path

import os, sys
def splitall(path):
    allparts = []
    while 1:
        parts = os.path.split(path)
        if parts[0] == path:  # sentinel for absolute paths
            allparts.insert(0, parts[0])
            break
        elif parts[1] == path: # sentinel for relative paths
            allparts.insert(0, parts[1])
            break
        else:
            path = parts[0]
            allparts.insert(0, parts[1])
    return allparts

def read_json_report(json_file_path):
    f = open(json_file_path)
    json_data = json.load(f)
    return json_data


def read_data(file_path, start, end):
    f = open(file_path)
    f.seek(start)
    data = f.read(end-start)
    json_data = '{' + data + '}'
    return json.loads(json_data)


def delete_json_report(json_file_path):
    os.remove(json_file_path)


def read_md5(file_path, index):
    target_idx = index['target']
    data = read_data(file_path, target_idx['start'], target_idx['end'])

    return data['target']['file']['md5']


def process_strings(md5, file_path, index):
    strings_idx = index['strings']
    data = read_data(file_path, strings_idx['start'], strings_idx['end'])

    strings = "\t".join(data['strings'])
    #print(f'md5: {md5} strings: {strings}')


def process_imports(md5, file_path, index):
    imports_idx = index['imports']
    data = read_data(file_path, imports_idx['start'], imports_idx['end'])
    print_count = 0

    try:
        for an_import_list in data['pe_imports']:
            dll = an_import_list['dll'].lower()
            for an_import in an_import_list['imports']:
                import_name = an_import['name']
                if import_name is not None:
                    if print_count <= PRINT_MAX:
                        #print(f'Imports md5: {md5} dll: {dll} import: {import_name}')
                        
                        print_count += 1
    except Exception as ex:
        print('process_imports error', ex)


def process_calls(md5, file_path, index):
    processes_idx = index['processes']
    data = read_data(file_path, processes_idx['start'], processes_idx['end'])
    print_count = 0

    sequence = 0
    try:
        for a_processes_list in data['processes']:
            process_path = a_processes_list['process_path']
            this = os.path.splitext(file_path)
            this1 = splitall(this[0])
            #print(process_path, "PROCESSSSS before")
            #print(r"C:\Users\cucko\AppData\Local\Temp\VirusShare_"+this1[5]+".exe"+" TRYING TO SEE")
            if this1[5]+".exe" in process_path :
                continue
            #print(process_path, "PROCESSSSS")
            calls = a_processes_list['calls']
            for a_call in calls:
                if sequence > 10000:
                    break

                api = a_call['api'].lower()
                time = a_call['time']
                arguments = a_call['arguments']
                
                if print_count <= PRINT_MAX:
                    #print(f'Calls md5: {md5} api: {api} time: {time}')
                    #print(f'api: {api} arguments: {arguments}')
                    #print(f'api: {api})
                    print_count += 1
                sequence += 1
    except Exception as ex:
        print(ex)


def process_api_stats(md5, file_path, index):
    apistats_idx = index['apistats']
    data = read_data(file_path, apistats_idx['start'], apistats_idx['end'])
    print_count = 0

    try:
        api_stats = data['apistats']
        keys = api_stats.keys()
        for key in keys:
            print(api_stats[key], file = report1File );
            stats_obj = api_stats[key]
            calls = stats_obj.keys()
            for call in calls:
                
                count = stats_obj[call]
                print(call, file = reportFile)
                #if (call == ())
                if print_count <= PRINT_MAX:
                    #print(call)
                    #print(f'API Stats md5: {md5} call: {call} count: {count}')
                    data_row.append(call)
                    data_row.append(count)
                    print_count += 1
    except Exception as ex:
        print('process_api_stats error', ex)


def read_index(path):
    parts = os.path.splitext(path)
    idx = parts[0] + '.idx'
    f = open(idx)
    return json.load(f)


def process_zip_file(file_path):
    print('Processing', file_path)
    #data_row.append(1)
    json_file_path = file_path
    index = read_index(json_file_path)
    md5 = read_md5(json_file_path, index)
    #process_strings(md5, json_file_path, index)
    #process_imports(md5, json_file_path, index)
    process_calls(md5, json_file_path, index)
    process_api_stats(md5, json_file_path, index)
    #df = pd.DataFrame(data_row)
    #print(df)
    with open('mal1.csv', 'a') as f_object:
        writer_object = writer(f_object)
        writer_object.writerow(data_row)
        f_object.close()
    #delete_json_report(json_file_path)


def read_zip_files(root):
    files = []
    for dir_name, sub_dir_list, file_list in os.walk(root):
        for file_name in file_list:
            file_path = os.path.join(dir_name, file_name)
            if 'json' in file_path:
                files.append(file_path)

    return files


zip_files = read_zip_files(os.getcwd() + os.sep + 'reports')
for zip_file in zip_files:
    #parts = os.path.splitext(zip_file)
    #zip_file = parts[0]
    process_zip_file(zip_file)
    data_row = []

#process_zip_file(sys.argv[1])

